{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8340d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-ilshapiro/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-04 00:13:09 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 00:13:11,318\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import socket\n",
    "import atexit\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Wait until the server is up\n",
    "def wait_until_ready(port=8080, timeout=30):\n",
    "    start = time.time()\n",
    "    while time.time() - start < timeout:\n",
    "        try:\n",
    "            with socket.create_connection((\"localhost\", port), timeout=1):\n",
    "                return\n",
    "        except OSError:\n",
    "            time.sleep(0.2)\n",
    "    raise TimeoutError(\"Server did not start in time.\")\n",
    "\n",
    "def start_coq_verification_server():\n",
    "    subprocess.run([\n",
    "            \"docker\", \"run\", \"--rm\", \"-d\",\n",
    "            \"--name\", \"coqstoq-server\",\n",
    "            \"-p\", \"8080:8080\",\n",
    "            \"coqstoq-full\",\n",
    "            \"poetry\", \"run\", \"python3\",\n",
    "            \"coqstoq/checker_server/server.py\", \"test\", \"77785\", \".\"\n",
    "        ], check=True)\n",
    "\n",
    "    # Stop the server automatically when the script ends\n",
    "    atexit.register(lambda: subprocess.run([\"docker\", \"stop\", \"coqstoq-server\"]))\n",
    "    wait_until_ready()\n",
    "\n",
    "def check_proof(proof: str) -> dict:\n",
    "    \"\"\"\n",
    "    Start the verification server if necessary, then POST the given proof.\n",
    "    Returns the parsed JSONâ€‘RPC response as a Python dict.\n",
    "    \"\"\"\n",
    "    start_coq_verification_server()\n",
    "    wait_until_ready()\n",
    "\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"method\":  \"check_proof\",\n",
    "        \"params\":  {\"proof\": proof},\n",
    "        \"id\":      1\n",
    "    }\n",
    "\n",
    "    r = requests.post(f\"http://localhost:8080\", json=payload, timeout=30)\n",
    "    r.raise_for_status()          # raise if HTTP error\n",
    "    return r.json()               # e.g. {\"result\": {\"score\": 1, \"messages\": []}, \"id\": 1, \"jsonrpc\": \"2.0\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e2286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Coqstoq verification server...\n",
      "Server is running and ready!\n",
      "Loading validation data...\n",
      "Loading tokenizer and checkpoint from /home/t-ilshapiro/CoqStoq/fstarcoq-qwq-32b-singleturn-sft... INFO 07-04 00:16:23 [config.py:823] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 07-04 00:16:23 [config.py:1946] Defaulting to use mp for distributed inference\n",
      "INFO 07-04 00:16:23 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 07-04 00:16:25 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 07-04 00:16:25 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/home/t-ilshapiro/CoqStoq/fstarcoq-qwq-32b-singleturn-sft', speculative_config=None, tokenizer='/home/t-ilshapiro/CoqStoq/fstarcoq-qwq-32b-singleturn-sft', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/t-ilshapiro/CoqStoq/fstarcoq-qwq-32b-singleturn-sft, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 07-04 00:16:25 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 07-04 00:16:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_da05c996'), local_subscribe_addr='ipc:///tmp/c2798d29-fa66-415e-8773-86f6d804da54', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 07-04 00:16:25 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x78a1b6cc7eb0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:16:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3952d399'), local_subscribe_addr='ipc:///tmp/381b9d39-695e-4d9a-b0c7-e97e965436be', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 07-04 00:16:25 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x78a1b6cc5150>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:16:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4c110c8c'), local_subscribe_addr='ipc:///tmp/02614315-22e1-4c4f-95da-ba221fc42024', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:16:26 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "INFO 07-04 00:16:26 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:16:26 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 07-04 00:16:26 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:16:26 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/t-ilshapiro/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 07-04 00:16:26 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/t-ilshapiro/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:16:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_1c8746b9'), local_subscribe_addr='ipc:///tmp/0da92d12-6d30-43de-bc68-37deba709926', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:16:26 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m WARNING 07-04 00:16:26 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:16:26 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:16:26 [gpu_model_runner.py:1595] Starting to load model /home/t-ilshapiro/CoqStoq/fstarcoq-qwq-32b-singleturn-sft...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m WARNING 07-04 00:16:26 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:16:26 [gpu_model_runner.py:1595] Starting to load model /home/t-ilshapiro/CoqStoq/fstarcoq-qwq-32b-singleturn-sft...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:16:27 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:16:27 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:16:27 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-04 00:16:27 [cuda.py:252] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:00<00:05,  2.52it/s]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:00<00:05,  2.02it/s]\n",
      "Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:01<00:05,  1.88it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:02<00:05,  1.91it/s]\n",
      "Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:02<00:03,  2.30it/s]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:02<00:03,  2.17it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:03<00:03,  2.00it/s]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 8/14 [00:03<00:03,  1.95it/s]\n",
      "Loading safetensors checkpoint shards:  64% Completed | 9/14 [00:04<00:02,  1.87it/s]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 10/14 [00:05<00:02,  1.86it/s]\n",
      "Loading safetensors checkpoint shards:  79% Completed | 11/14 [00:05<00:01,  1.88it/s]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 12/14 [00:06<00:01,  1.85it/s]\n",
      "Loading safetensors checkpoint shards:  93% Completed | 13/14 [00:06<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:16:34 [default_loader.py:272] Loading weights took 7.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:07<00:00,  1.85it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:07<00:00,  1.93it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:16:34 [default_loader.py:272] Loading weights took 7.36 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:16:35 [gpu_model_runner.py:1624] Model loading took 30.7118 GiB and 7.404882 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:16:35 [gpu_model_runner.py:1624] Model loading took 30.7118 GiB and 7.663283 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:16:48 [backends.py:462] Using cache directory: /home/t-ilshapiro/.cache/vllm/torch_compile_cache/2c592b1f5b/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:16:48 [backends.py:472] Dynamo bytecode transform time: 13.31 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:16:48 [backends.py:462] Using cache directory: /home/t-ilshapiro/.cache/vllm/torch_compile_cache/2c592b1f5b/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:16:48 [backends.py:472] Dynamo bytecode transform time: 13.37 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:17:00 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 11.271 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:17:00 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 11.434 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:17:10 [monitor.py:34] torch.compile takes 13.31 s in total\n",
      "INFO 07-04 00:17:10 [monitor.py:34] torch.compile takes 13.37 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:17:11 [gpu_worker.py:227] Available KV cache memory: 37.94 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:17:11 [gpu_worker.py:227] Available KV cache memory: 37.94 GiB\n",
      "INFO 07-04 00:17:12 [kv_cache_utils.py:715] GPU KV cache size: 310,784 tokens\n",
      "INFO 07-04 00:17:12 [kv_cache_utils.py:719] Maximum concurrency for 16,384 tokens per request: 18.97x\n",
      "INFO 07-04 00:17:12 [kv_cache_utils.py:715] GPU KV cache size: 310,784 tokens\n",
      "INFO 07-04 00:17:12 [kv_cache_utils.py:719] Maximum concurrency for 16,384 tokens per request: 18.97x\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:17:42 [custom_all_reduce.py:196] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:17:54 [custom_all_reduce.py:196] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1448855)\u001b[0;0m INFO 07-04 00:17:54 [gpu_model_runner.py:2048] Graph capturing finished in 43 secs, took 0.96 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1448852)\u001b[0;0m INFO 07-04 00:17:54 [gpu_model_runner.py:2048] Graph capturing finished in 43 secs, took 0.96 GiB\n",
      "INFO 07-04 00:17:54 [core.py:171] init engine (profile, create kv cache, warmup model) took 79.48 seconds\n",
      "Preparing prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 919/10396 [00:11<02:02, 77.17it/s] Token indices sequence length is longer than the specified maximum sequence length for this model (885702 > 131072). Running this sequence through the model will result in indexing errors\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 8114/10396 [01:42<00:34, 66.30it/s] "
     ]
    }
   ],
   "source": [
    "# Argument parsing\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"/home/t-ilshapiro/CoqStoq/fstarcoq-qwq-32b-singleturn-sft\") # path that points to the directory with the model name (e.g. fstarcoq-qwq-32b...)\n",
    "parser.add_argument(\"--sample_n\", type=int, default=1) # how many times we sample for each prompt (i.e. sample on same input)\n",
    "parser.add_argument(\"--temperature\", type=float, default=0.7)\n",
    "parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "parser.add_argument(\"--num_gpus\", type=int, default=2)\n",
    "args, _ = parser.parse_known_args() # patch for Jupyter notebooks\n",
    "\n",
    "print(\"Starting Coqstoq verification server...\")\n",
    "# Start the Docker server in detached mode\n",
    "\n",
    "print(\"Server is running and ready!\")\n",
    "\n",
    "# Load validation data\n",
    "print(\"Loading validation data...\")\n",
    "valid_data = []\n",
    "with open(\"coq-test-data.jsonl\") as file:\n",
    "    for line in file:\n",
    "        valid_data.append(json.loads(line))\n",
    "if args.debug:\n",
    "    valid_data = valid_data[:100]\n",
    "\n",
    "# Load tokenizer and vLLM engine\n",
    "print(f\"Loading tokenizer and checkpoint from {args.model_name}... \", end=\"\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "llm = LLM(model=args.model_name, dtype=\"bfloat16\", max_model_len=16384, tensor_parallel_size=args.num_gpus)\n",
    "\n",
    "# Prepare prompts\n",
    "print(\"Preparing prompts...\")\n",
    "prompts = []\n",
    "prompt_to_index = []  # (datum_idx, sample_idx)\n",
    "for datum_idx, datum in enumerate(tqdm(valid_data)):\n",
    "    prompt = datum[\"user_prompt\"]\n",
    "    if len(tokenizer(prompt).input_ids) > 8192:\n",
    "        continue\n",
    "    for sample_idx in range(args.sample_n):\n",
    "        prompts.append(prompt)\n",
    "        prompt_to_index.append((datum_idx,sample_idx))\n",
    "\n",
    "# Generate with vLLM\n",
    "print(f\"Sampling responses... {args.sample_n} samples per prompt, temp={args.temperature}\")\n",
    "sampling_params = SamplingParams(temperature=args.temperature, max_tokens=16384, n=1)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Organize responses into valid_data\n",
    "for datum in valid_data:\n",
    "    datum[\"model_generated_response\"] = [] # length of this list will be sample_n\n",
    "\n",
    "for output, (datum_idx, _) in zip(outputs, prompt_to_index):\n",
    "    response = output.outputs[0].text\n",
    "    if \"<answer>\" in response and \"</answer>\" in response:\n",
    "        valid_data[datum_idx][\"model_generated_response\"].append(response) # recall datum_idx is the line number in the jsonl file\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
